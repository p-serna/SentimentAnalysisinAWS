{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Above we mentioned that `review_to_words` method removes html formatting and allows us to tokenize the words found in a review, for example, converting *entertained* and *entertaining* into *entertain* so that they are treated as though they are the same word. What else, if anything, does this method do to the input?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** After removing HTML tags and before extracting the stem of the words to tokenize them, it first transforms the text to lowercase and substitute every character that is not a letter or a number to a space (line 13, using regular expressions). Then it splits the text in words, by using the space as a separation mark, and creates a list with them (line 14). And just before keeping the stem and tokenizing the words, it proceeds to remove stopwords from the list, such as \"a\" or \"the\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What does  this 1  do to characters such as the interrogation marK  '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"[^a-zA-Z0-9]\", \" \", 'What does \"this-1\" do to characters such as the interrogation marK?.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What are the five most frequently appearing (tokenized) words in the training set? Does it makes sense that these words appear frequently in the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The most frequent is \"movie\" and words that use it as stem. Then it is followed by film, one, like and time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movi', 2), ('film', 3), ('one', 4), ('like', 5), ('time', 6)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Use this space to determine the five most frequently appearing words in the training set.\n",
    "list(word_dict.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** In the cells above we use the `preprocess_data` and `convert_and_pad_data` methods to process both the training and testing set. Why or why not might this be a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** They should have both the preprocessing procedure to avoid different formats or preparation issues which may bias the predictions of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How does this model compare to the XGBoost model you created earlier? Why might these two models perform differently on this dataset? Which do *you* think is better for sentiment analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "Well, funnily it is slightly worse (with XGBoost we obtained roughly 0.87), but this might be due to a fluctuation or not enough training. Long-Short Term Memory modules are in principle used quite frequently in text analysis and other text applications. Probably tuning hyperparameters would give us access to a model with better performance. Instead of that, I tried to explore other architectures by stacking 2 or 3 LSTMs (an example in model_stack.py). While this helped to decrease the training loss, the performance in the test set was worse, accuracy decreased to 0.8 or 0.81.\n",
    "\n",
    "Overall, if one has to take into account the ammount of training time, it seems XGBoost was quite efficient in providing good results, no wonder it won so many contests. To answer the last question we should then constrain it by defining what better is. If it just accuracy in the test set, RNNs have enough complexity to produce good results, but they are prone to overfit and we need large data sets to wavoid it. Without any hyperparameter tuning, for this particular task XGBoost was slightly better (maybe marginally). In efficiency, XGBoost is the clear winner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your web app is working, trying playing around with it and see how well it works.\n",
    "\n",
    "**Question**: Give an example of a review that you entered into your web app. What was the predicted sentiment of your example review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** I tried to find the limits of the app, and it is actually pretty good, it is able to even catch some ironies. One that I tried and I particularly enjoyed was the review by Kaitlyn Tiffany and Lizzie Plaugic (The Verge) of the Emoji movie:\n",
    "\n",
    "\"This is a movie about how words aren't cool, but you can still expect a girl to fall at your feet in response to mild wordplay. Please keep up. Or throw whatever device you’re reading this on into the ocean. Send me a postcard ... tell me what it’s like to be free.\" — \n",
    "\n",
    "It considered it a good review!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
